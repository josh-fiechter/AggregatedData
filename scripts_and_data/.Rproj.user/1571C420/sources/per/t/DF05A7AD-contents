---
title: "Supplemental Online Materials"
author: "Josh Fiechter"
date: "2025-05-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message = FALSE, warning = FALSE}
library(tidyverse)
library(here)
library(grafify)
library(brms)
library(posterior)
library(tidybayes)
ncores <- parallel::detectCores()
options(mc.cores = ncores)
set.seed(123)
```

## Section 1: Fit candidate models to testing-effect data

### 1.1: Read in data

```{r message = FALSE, warning = FALSE}
dat <- read_csv(here("testing_effects.csv"))
```

I'll fit the `Aggregate` model first, and so I need to average over participants.

```{r message = FALSE, warning = FALSE}
dat %>%
  group_by(Feedback, Experiment, Sub, Condition) %>%
  summarize(Correct = mean(Correct),
            InitRet = mean(InitRet),
            NumPracCorrect = mean(NumPracCorrect),
            NumPracTrial = mean(NumPracTrial)) %>%
  group_by(Feedback, Experiment, Sub) %>%
  summarize(test_effect = Correct[Condition == "Ret"]-Correct[Condition == "Study"],
            InitRet = InitRet[Condition == "Ret"],
            NumPracCorrect = NumPracCorrect[Condition == "Ret"],
            NumPracTrial = NumPracTrial[Condition == "Ret"]) %>%
  ungroup() %>%
  mutate(PropPracCorrect = NumPracCorrect/NumPracTrial,
         InitRet_z = scale(InitRet)[,1],
         PropPracCorrect_z = scale(PropPracCorrect)[,1]) %>%
  ungroup() -> dat_agg
```

### 1.2: Fit `Aggregate` model

I'll use the `brms`^1^ R package, which provides a user-friendly means of declaring Bayesian models that will ultimately be estimated in Stan^2^.

This model will be a general linear model (i.e., I will estimate parameters of the Gaussian distribution) with no multilevel structure. I will model `test_effect`, which is the average by-subject testing effect (i.e., differential final-test accuracy between tested and restudied items). I will include seven predictors in this model:

1. `Feedback`, that is, a binary indicator of whether corrective feedback was provided during the practice phase.

2. `InitRet_z`, that is, z-scored initial retrieval success for items that were practiced via testing during the practice phase.

3. `PropPracCorrect_z`, that is, z-scored overall proportion of retrieval success for items that were practiced via testing during the practice phase.

4. The `Feedback:InitRet_z` interaction.

5. The `Feedback:PropPracCorrect_z` interaction.

6. The `InitRet_z:PropPracCorrect_z` interaction.

7. The `Feedback:InitRet_z:PropPracCorrect_z` interaction.

The incorporation of `InitRet_z` and `PropPracCorrect_z` and all possible interactions that are generated by their inclusion results in superior model fits (i.e., determined by a LOO CV^3^ comparison) relative to a model that includes only `Feedback` as a predictor.

I'll place unit-Cauchy priors on all parameters (i.e., `Cauchy(0,1)`). These priors have two desirable properties:

1. They are <i>weakly informative</i>; that is, they will restrict the parameter space that is explored by the Hamiltonian Monte Carlo algorithm that is leveraged in Stan, but an acceptably broad spectrum of plausible estimates will still be explored.

2. They are appropriate for calculating Bayes factors for discrete and scaled predictors (i.e., such as the ones that are used in this model).

```{r message = FALSE, warning = FALSE}
bf_1 <- bf(test_effect ~ 0 + Intercept + Feedback*InitRet_z*PropPracCorrect_z,
          family = gaussian)

prior_1 <- prior(cauchy(0,1), class = b)

fit_Aggregate <- brm(bf_1,
                     data = dat_agg,
                     prior = prior_1,
                     sample_prior = TRUE,
                     iter = 5000,
                     seed = 123,
                     save_pars = save_pars(all = TRUE),
                     threads = threading(floor(ncores/4)))
```

#### 1.2.1: Calculate Bayes factors and standardized effect sizes for `Aggregate` model

I'll wait to look at estimates until I have fits for all three candidate models. For now, I'll save two Bayes factors that are calculated via Savage-Dickey ratios^4^.

```{r message = FALSE, warning = FALSE}
bf_Aggregate <- hypothesis(fit_Aggregate, 
                      c("Intercept = 0", 
                        "Intercept + FeedbackTRUE = 0"))
```

As a final step for this model, I'll generate standardized effect sizes (i.e., <i>d</i>). In the case of the `Aggregate` model, I'll divide posterior draws for our effects of interest by draws pertaining to `Sigma`^5^.

```{r message = FALSE, warning = FALSE}
d_agg <-
  as_draws_df(fit_Aggregate) %>%
  mutate(fb = b_Intercept + b_FeedbackTRUE,
         d_fb = fb/sigma,
         d_nofb = b_Intercept/sigma)

d_agg_fb <- c(mean(d_agg$d_fb),
              sd(d_agg$d_fb),
  quantile(d_agg$d_fb, c(.025, .975)))
  
d_agg_nofb <- c(mean(d_agg$d_nofb),
                sd(d_agg$d_nofb),
  quantile(d_agg$d_nofb, c(.025, .975)))
```

### 1.3: Fit `Intercept` model

I'll next model the unaggregated data via a multivariate multilevel generalized linear model. This `Intercept` model will only estimate group-level intercepts and will not include any group-level effects.

As a preliminary step, I need to prep the data by adding indicators for the relevant subsets of observations pertaining to the `InitRet` and `NumPracCorrect` response variables (see below for more details on those variables).

```{r message = FALSE, warning = FALSE}
dat %>%
  mutate(set1 = ifelse(Condition == "Ret",
                       TRUE,
                       FALSE),
         set2 = ifelse(Condition == "Ret" & NumPracTrial > 1, 
                       TRUE, 
                       FALSE)) -> dat
```

I will model three outcomes of interest:

1. `Correct`, i.e., final-test accuracy. This outcome will be modeled via Bernoulli regression.

2. `InitRet`, i.e., initial retrieval success on the first practice round for observations pertaining to the retrieval-practice condition. This outcome will also be modeled via Bernoulli regression.

3. `NumPracCorrect`, i.e., number of correct responses on practice tests for observations pertaining to the retrieval-practice condition in a multi-practice round experiment. This outcome will be modeled via binomial regression.

I will incorporate three predictors for `Correct`:

1. `Condition`, i.e., the retrieval practice or restudy condition.

2. `Feedback`, i.e., whether feedback was provided during the practice phase.

3. The `Condition:Feedback` interaction.

For the other two response variables, I will only include `Feedback` as a predictor. These responses pertain exclusively to observations that correspond to the retrieval-practice condition and so `Condition` is not a valid predictor.

This model includes four grouping variables:

1. `Sub`, i.e., participants.

2. `Cue`, i.e., items.

3. `TrialGr`, i.e., test trial.

4. `Experiment`.

Note that `Sub` and `Experiment` may be grouped by whether `Feedback` was provided at practice, and so I will estimate separate group-level effects for those two distinct sub-populations of participants and experiments.

Correlations between group-level estimates across all three response variables are captured by the `r1`, `r2`, `r3`, and `r4` terms.

This model that incorporates three response variables is favored in an approximated LOO-CV comparison relative to models that include only two or else one response variable.

Once again, I'll place unit-Cauchy priors on all parameters.

```{r message = FALSE, warning = FALSE}
bf_1 <- bf(Correct ~ Condition*Feedback +
             (1|r1|gr(Sub, by = Feedback)) +
             (1|r2|Cue) +
             (1|r3|TrialGr) +
             (1|r4|gr(Experiment, by = Feedback)),
           family = bernoulli)

bf_2 <- bf(InitRet|subset(set1) ~ Feedback +
             (1|r1|gr(Sub, by = Feedback)) +
             (1|r2|Cue) +
             (1|r3|TrialGr) +
             (1|r4|gr(Experiment, by = Feedback)),
           family = bernoulli)

bf_3 <- bf(NumPracCorrect|subset(set2) + trials(NumPracTrial) ~ Feedback +
             (1|r1|gr(Sub, by = Feedback)) +
             (1|r2|Cue) +
             (1|r3|TrialGr) +
             (1|r4|gr(Experiment, by = Feedback)),
           family = binomial)

prior_mv <- prior(cauchy(0,1), class = b, resp = Correct) +
  prior(cauchy(0,1), class = b, resp = InitRet) +
  prior(cauchy(0,1), class = b, resp = NumPracCorrect)

# Shuffle the data to facilitate estimation across multiple threads
dat[sample(nrow(dat)),] -> dat

fit_Intercept <- brm(bf_1 + bf_2 + bf_3 + set_rescor(FALSE),
                     data = dat,
                     prior = prior_mv,
                     sample_prior = TRUE,
                     iter = 5000,
                     seed = 123,
                     control = list(adapt_delta = .999),
                     save_pars = save_pars(all = TRUE),
                     threads = threading(floor(ncores/4)))
```

#### 1.3.1: Calculate Bayes factors and standardized effect sizes for `Intercept` model

I will once again save two Bayes factors that correspond to testing effects with and without feedback.

```{r message = FALSE, warning = FALSE}
bf_Intercept <- hypothesis(fit_Intercept,
                           c("Correct_ConditionStudy = 0",
                             "Correct_FeedbackTRUE = 
                        Correct_FeedbackTRUE + 
                        Correct_ConditionStudy + 
                        Correct_ConditionStudy:FeedbackTRUE"))
```

And I will once again use posterior draws to estimate a standardized effect size, <i>d</i>. In the case of the `Intercept` model, I'll divide posterior draws for our effects of interests by sums of draws pertaining to group-level `sd` coefficients^5^. I will only use `sd` coefficients that correspond to the `Correct` outcome variable.

```{r message = FALSE, warning = FALSE}
d_int <-
  as_draws_df(fit_Intercept, variable = c("^b_", "sd_"), regex = TRUE) %>%
  select(contains("_Correct")) %>%
  mutate(across(contains("sd"), ~.^2)) %>%
  mutate(fb = b_Correct_ConditionStudy + `b_Correct_ConditionStudy:FeedbackTRUE`,
         d_fb = fb/sqrt(rowSums(.[5:10]))*-1,
         d_nofb = b_Correct_ConditionStudy/sqrt(rowSums(.[5:10]))*-1)

d_int_fb <- c(mean(d_int$d_fb),
              sd(d_int$d_fb),
  quantile(d_int$d_fb, c(.025, .975)))

d_int_nofb <- c(mean(d_int$d_nofb),
                sd(d_int$d_nofb),
  quantile(d_int$d_nofb, c(.025, .975)))
```

### 1.4: Fit `Maximal` model

Finally, I will fit a multivariate multilevel model that incorporates maximal group-level effects. 

This `Maximal` model incorporates all the features of the `Intercept` model, plus all possible group-level effects of `Feedback`, `Condition` (for the `Correct` outcome), and `Feedback:Condition` (for the `Correct` outcome). 

```{r message = FALSE, warning = FALSE}
bf_1 <- bf(Correct ~ Condition*Feedback +
             (Condition|r1|gr(Sub, by = Feedback)) +
             (Condition*Feedback|r2|Cue) +
             (Condition*Feedback|r3|TrialGr) +
             (Condition|r4|gr(Experiment, by = Feedback)),
           family = bernoulli)

bf_2 <- bf(InitRet|subset(set1) ~ Feedback +
             (1|r1|gr(Sub, by = Feedback)) +
             (Feedback|r2|Cue) +
             (Feedback|r3|TrialGr) +
             (1|r4|gr(Experiment, by = Feedback)),
           family = bernoulli)

bf_3 <- bf(NumPracCorrect|subset(set2) + trials(NumPracTrial) ~ Feedback +
             (1|r1|gr(Sub, by = Feedback)) +
             (Feedback|r2|Cue) +
             (Feedback|r3|TrialGr) +
             (1|r4|gr(Experiment, by = Feedback)),
           family = binomial)

prior_mv <- prior(cauchy(0,1), class = b, resp = Correct) +
  prior(cauchy(0,1), class = b, resp = InitRet) +
  prior(cauchy(0,1), class = b, resp = NumPracCorrect)

fit_Maximal <- brm(bf_1 + bf_2 + bf_3 + set_rescor(FALSE),
               data = dat,
               prior = prior_mv,
               sample_prior = TRUE,
               iter = 5000,
               seed = 123,
               control = list(adapt_delta = .99),
               save_pars = save_pars(all = TRUE),
               threads = threading(floor(ncores/4)))
```

#### 1.4.1: Calculate Bayes factors and standardized effect sizes for `Maximal` model

I will once again save two Bayes factors that correspond to testing effects with and without feedback.

```{r message = FALSE, warning = FALSE}
bf_Maximal <- hypothesis(fit_Maximal,
                      c("Correct_ConditionStudy = 0",
                        "Correct_FeedbackTRUE = 
                        Correct_FeedbackTRUE + 
                        Correct_ConditionStudy + 
                        Correct_ConditionStudy:FeedbackTRUE"))
```

And I will once again use posterior draws to estimate a standardized effect size, <i>d</i>. Once again, I'll divide posterior draws for our effects of interests by sums of draws pertaining to group-level `sd` coefficients. I will also once again only use `sd` coefficients that correspond to the `Correct` outcome variable.

```{r message = FALSE, warning = FALSE}
d_max <-
  as_draws_df(fit_Maximal, variable = c("^b_", "sd_"), regex = TRUE) %>%
  select(contains("_Correct")) %>%
  mutate(across(contains("sd"), ~.^2)) %>%
  mutate(fb = b_Correct_ConditionStudy + `b_Correct_ConditionStudy:FeedbackTRUE`,
         d_fb = fb/sqrt(rowSums(.[5:20]))*-1,
         d_nofb = b_Correct_ConditionStudy/sqrt(rowSums(.[5:20]))*-1)

d_max_fb <- c(mean(d_max$d_fb),
              sd(d_max$d_fb),
              quantile(d_max$d_fb, c(.025, .975)))

d_max_nofb <- c(mean(d_max$d_nofb),
                sd(d_max$d_nofb),
                quantile(d_max$d_nofb, c(.025, .975)))
```

#### 1.4.2: Compare `Intercept` and `Maximal` models with respect to `Correct`

Previous work has suggested that maximal group-level effects are appropriate as long as model comparisons suggest that a simpler model is not clearly preferred^6^. Such a scenario will arise if, for example, there is insufficient group-level variance to warrant explicit estimation of such quantities^7^. 

To ensure that the `Maximal` model is indeed the appropriate model for these data, I will conduct approximated LOO CV pertaining to the `Correct` response.

```{r, message = FALSE, warning = FALSE}
loo_Maximal <- loo(fit_Maximal, resp = "Correct")
loo_Intercept <- loo(fit_Intercept, resp = "Correct")
```

```{r, message = FALSE, warning = FALSE}
loo_compare(loo_Maximal, loo_Intercept)
```

The expected log pointwise predictive density (i.e., `elpd_diff`) is roughly four times the standard error of that quantity (i.e., `se_diff`), and so the `Maximal` model is clearly favored over the `Intercept` model for these data.

## Section 2: Compile estimates from the candidate models

### 2.1: Estimated testing effects

To compare outputs of all three models, I'll transform posterior estimates from the logit-linked `Intercept` and `Maximal` models.

First, I'll generate population-level estimates for all three models. (This "population-level" distinction applies only to the multilevel models.)

```{r message = FALSE, warning = FALSE}
source("03_Transformed_posterior_fxns.R")

dat_population <- transformed_groups(dat)
```

Next, I'll generate group-level estimates for all the grouping variables that are included in the `Intercept` and `Maximal` models.

```{r message = FALSE, warning = FALSE}
## Subject-level estimates
dat_subs <- transformed_groups(dat, "Sub")

## Item-level estimates
dat_items <- transformed_groups(dat, "Cue")

## Trial-level estimates
dat_trials <- transformed_groups(dat, "TrialGr")

## Experiment-level estimates
dat_experiments <- transformed_groups(dat, "Experiment")

## Put all group-level estimates together
dat_items %>%
  bind_rows(dat_subs,
            dat_trials,
            dat_experiments) -> dat_groups
```

The values in `dat_groups` are presented in Table 1 in the main manuscript.

```{r message = FALSE, warning = FALSE}
dat_groups
```

Next, I'll plot the population-level estimates from each model. This plot is Figure 1 in the main manuscript.

```{r message = FALSE, warning = FALSE}
dat_population %>%
  rename("Estimate" = mean, "CI.Lower" = q2.5, "CI.Upper" = q97.5) %>%
  mutate(Feedback = ifelse(Feedback == TRUE, "Feedback", "No Feedback")) %>%
  mutate(N = case_when(
    model == "Aggregate" & Feedback == "Feedback" ~ 313,
    model == "Aggregate" & Feedback == "No Feedback" ~ 312,
    model != "Aggregate" & Feedback == "Feedback" ~ 3512,
    model != "Aggregate" & Feedback == "No Feedback" ~ 3520
  )) %>%
  arrange(Feedback, model) -> dat_population
```

```{r message = FALSE, warning = FALSE}
ggplot(dat_population, aes(y = Estimate, 
                  ymin = CI.Lower, 
                  ymax = CI.Upper, 
                  x = model)) +
  geom_hline(yintercept = 0, 
             color = "red",
             linetype = "dotted") +
  geom_errorbar(aes(color = model),
                width = .3) +
  geom_point(aes(color = model,
                 fill = model,
                 size = N),
             alpha = .5) +
  geom_point(aes(color = model),
             shape = 4) +
  scale_color_grafify() +
  facet_wrap(~Feedback, 
             nrow = 1) +
  ylab("Testing effect") +
  xlab("Model") +
  theme_classic() +
  theme(panel.border = element_rect(fill = NA, 
                                    color = "black"),
        panel.grid.major.y = element_line(linetype = "dotted", 
                                          color = "light gray"),
        legend.position = "none",
        axis.text.x = element_text(angle = 45, 
                                   hjust = 1))
```

### 2.2: Estimated standardized effects and Bayes factors

Next, I'll compile the standardized effect sizes from each model.

```{r message = FALSE, warning = FALSE}
dat_d <- bind_rows(as_tibble(t(cbind(d_agg_fb, d_int_fb, d_max_fb))),
                   as_tibble(t(cbind(d_agg_nofb, d_int_nofb, d_max_nofb)))) %>%
  rename("Estimate" = V1, "SD" = V2, "CI.Lower" = `2.5%`, "CI.Upper" = `97.5%`) %>%
  mutate(Model = c(replicate(2, c("Aggregate", "Intercept", "Maximal"))),
         Feedback = c(replicate(3, "Feedback"), 
                      replicate(3, "No Feedback"))) %>%
  mutate(N = case_when(
    Model == "Aggregate" & Feedback == "Feedback" ~ 313,
    Model == "Aggregate" & Feedback == "No Feedback" ~ 312,
    Model != "Aggregate" & Feedback == "Feedback" ~ 3512,
    Model != "Aggregate" & Feedback == "No Feedback" ~ 3520
  )) %>%
  arrange(Feedback)
```

```{r message = FALSE, warning = FALSE}
dat_d
```

Finally, I'll compile Bayes factors for testing effects, with and without feedback, from each of the three candidate models.

```{r message = FALSE, warning = FALSE}
dat_bf <- as_tibble(c(1/bf_Aggregate$hypothesis$Evid.Ratio,
                      1/bf_Intercept$hypothesis$Evid.Ratio,
                      1/bf_Maximal$hypothesis$Evid.Ratio)) %>%
  mutate(Model = c(replicate(2, "Aggregate"), 
                   replicate(2, "Intercept"), 
                   replicate(2, "Maximal")),
         Feedback = c(replicate(3, c("No Feedback", "Feedback")))) %>%
  arrange(Feedback)
```

```{r message = FALSE, warning = FALSE}
dat_bf
```

These estimates of <i>d</i> and Bayes factors are presented in Table 2 in the main manuscript.

## Section 3: Simulate the reproducibility of `Aggregate`, `Intercept`, and `Maximal` models

The purpose of this simulation is to provide supporting evidence for the claim that findings that are generated from a maximal multilevel model are more reproducible than those that are generated by an aggregate or random-intercept model. I'm operationalizing reproducibility as <i>P</i>(estimate capture), or the rate at which a given estimate is captured by the 95% credible interval of a reference study.

These simulations are intended to approximate a testing-effect study in which the testing and restudy `Condition` each yield `Correct = 0.30`, without `Feedback`, and `0.43` and `0.30`, with `Feedback`. Each study entails 120 participants (i.e., `subject`), a within-participant manipulation of `Condition` (i.e., restudy or test), and a between-participant manipulation of `Feedback` (provided or not).

I will run 12 sets of simulations. Each set will consist of 100 studies. They will vary, first, by the extent to which participants and items vary about the logit-transformed population means. For simplicity, I'm assuming uniform variance across those two entities, and my six values of sigma (i.e., `group_sd`) across the simulations are `c(2, 1.66, 1.33, 1, 0.66, 0.33)`. These values approximately align with the estimates of group-level variance from the `Maximal` model in my analyses above. Second, the simulations will vary by the number of items (i.e., `item = 12` or `36`) that are included.

I will once again evaluate three models. First, I will take the average testing effect by participant and model participant means in the `Aggregate` model.

```{r}
bf_1 <- bf(test_effect ~ Feedback,
                 family = gaussian)
```

Second, I will fit a multilevel `Intercept` model that excludes group-level predictors.

```{r}
bf_1 <- bf(Correct ~ Condition*Feedback +
                   (1|gr(subject, by = Feedback)) +
                   (1|item),
                 family = bernoulli)
```

Finally, I will fit a multilevel `Maximal` model that includes all possible group-level predictors.

```{r}
bf_1 <- bf(Correct ~ Condition*Feedback +
                   (Condition|gr(subject, by = Feedback)) +
                   (Condition*Feedback|item),
                 family = bernoulli)
```

The models that are evaluated here are simpler than the ones that are used for the analyses in the main manuscript. They don't include any additional covariates (in the case of the `Aggregate` model) or response variables (in the case of the `Intercept` and `Maximal` models) outside of `Correct`/`test_effect`, `Condition`, and `Feedback`. (Code for the simulations is located in the `05_Sims_fxns.R` script.)

```{r message = FALSE, warning = FALSE}
source("05_Sims_fxns.R")
```

```{r message = FALSE, warning = FALSE}
dat_12_1 <- sim_fits(100, 120, 12, 2)
dat_12_2 <- sim_fits(100, 120, 12, 1.66)
dat_12_3 <- sim_fits(100, 120, 12, 1.33)
dat_12_4 <- sim_fits(100, 120, 12, 1)
dat_12_5 <- sim_fits(100, 120, 12, .66)
dat_12_6 <- sim_fits(100, 120, 12, .33)

dat_36_1 <- sim_fits(100, 120, 36, 2)
dat_36_2 <- sim_fits(100, 120, 36, 1.66)
dat_36_3 <- sim_fits(100, 120, 36, 1.33)
dat_36_4 <- sim_fits(100, 120, 36, 1)
dat_36_5 <- sim_fits(100, 120, 36, .66)
dat_36_6 <- sim_fits(100, 120, 36, .33)
```

I'll merge all the data for summarizing.

```{r message = FALSE, warning = FALSE}
dat_12_1 %>%
  bind_rows(dat_12_2,
            dat_12_3,
            dat_12_4,
            dat_12_5,
            dat_12_6,
            dat_36_1, 
            dat_36_2, 
            dat_36_3,
            dat_36_4,
            dat_36_5,
            dat_36_6) -> dat_sims
```

In lieu of averaging, I'll aggregate the reproducibility rates from my simulations via a multivariate multilevel zero-one-inflated-beta (ZOIB) model with `group_sd` treated as a grouping variable.

```{r message = FALSE, warning = FALSE}
bf1 <- bf(mvbind(Aggregate_rep, Intercept_rep, Maximal_rep) ~ Feedback*n_items*model +
            (Feedback*n_items*model|r1|group_sd),
          family = zero_one_inflated_beta)

prior1 <- prior(cauchy(0,1), class = b, resp = Aggregaterep) +
  prior(cauchy(0,1), class = b, resp = Interceptrep) +
  prior(cauchy(0,1), class = b, resp = Maximalrep)

dat_sims <- dat_sims[sample(nrow(dat_sims)),]
dat_sims$n_items <- factor(dat_sims$n_items)

fit <- brm(bf1 + set_rescor(FALSE),
           prior = prior1,
           sample_prior = TRUE,
           data = dat_sims,
           init = 0,
           control = list(adapt_delta = .99),
           seed = 123,
           threads = threading(floor(ncores/4)))
```

Getting population-level estimates from the ZOIB model into a single data frame.

```{r message = FALSE, warning = FALSE}
## Visualize fits
dat_sims %>% 
  distinct(Feedback, n_items, model) %>%
  bind_cols(fitted(fit, resp = "Aggregaterep", newdata = ., re_formula = NA)) %>%
  unite("Aggregate_fits", Estimate:Q97.5) %>%
  bind_cols(fitted(fit, resp = "Interceptrep", newdata = ., re_formula = NA)) %>%
  unite("Intercept_fits", Estimate:Q97.5) %>%
  bind_cols(fitted(fit, resp = "Maximalrep", newdata = ., re_formula = NA)) %>%
  unite("Maximal_fits", Estimate:Q97.5) %>%
  pivot_longer(contains("fits"),
               names_to = "Replication model",
               values_to = "Estimates") %>%
  separate_wider_delim(Estimates,
                       delim = "_",
                       names = c("Estimate", "Error", "Q2.5", "Q97.5")) %>%
  mutate(across(Estimate:Q97.5, parse_number),
         Feedback = ifelse(Feedback == "no", 
                           "No Feedback",
                           "Feedback"),
         `Replication model` = str_remove_all(`Replication model`, "_fits")) -> dat_viz
```

And now I'll plot the findings aggregated over `group_sd`.

```{r message = FALSE, warning = FALSE}
ggplot(dat_viz, aes(x = model, y = Estimate)) +
  facet_wrap(~n_items + Feedback, nrow = 2) +
  geom_pointrange(aes(ymin = Q2.5, ymax = Q97.5,
                      color = `Replication model`,
                      fill = `Replication model`),
                  position = position_dodge(.5),
                  alpha = .5) +
  scale_color_grafify() +
  scale_fill_grafify() +
  ylab("P(Estimate capture)") +
  xlab("Reference model") +
  theme_classic() +
  theme(panel.border = element_rect(fill = NA, color = "black"),
        panel.grid.major.y = element_line(linetype = "dotted", color = "light gray"))
```

This plot demonstrates that the `Maximal` model exhibits the highest reproducibility over all sets of simulated studies. This superior reproducibility holds even when replication findings are generated by either of the `Aggregate` or `Intercept` models.

## References

1. Bürkner, P.-C. (2017). brms: An R package for Bayesian multilevel models using Stan. Journal 
of Statistical Software, 80, 1–28.

2. Stan Development Team. (2024). Stan Modeling Language Users Guide and Reference Manual, 
2.36. https://mc-stan.org

3. Vehtari, A., Gelman, A., & Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing, 27, 1413–1432.

4. Wagenmakers, E.-J., Lodewyckx, T., Kuriyal, H., & Grasman, R. (2010). Bayesian hypothesis 
testing for psychologists: A tutorial on the Savage–Dickey method. Cognitive Psychology, 60, 158–189.

5. Nalborczyk, L., Batailler, C., Lœvenbruck, H., Vilain, A., Bürkner, P.-C. (2019). An introduction to Bayesian multilevel models using brms: A case study of gender effects on vowel variability in standard Indonesian. Journal of Speech, Hearing, and Language Research, 62, 1225–1242. 

6. Oberauer, K. (2022). The importance of random slopes in mixed models for Bayesian hypothesis 
testing. Psychological Science, 33, 648–665.

7. Matuschek, H., Kliegl, R., Vasishth, S., Baayen, H., & Bates, D. (2017). Balancing Type I error
and power in linear mixed models. Journal of Memory and Language, 94, 305–315.

